{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"diffusion probabilistic model\" or \"denoising score matching model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diffusion model is designed to \n",
    "1. learn a probabilistic transformation that can be used to generate samples from a simple distribution (e.g., Gaussian) \n",
    "2. and transform them into samples that resemble the true data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Basic Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1.Probabilistic Transformation:\n",
    "\n",
    "The core idea of a diffusion model is to learn a probabilistic transformation that can map samples (from a simple distribution (e.g., Gaussian noise)) to samples (from a complex data distribution (e.g., images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2.Denoising Score Matching:\n",
    "\n",
    "The training of a diffusion model often involves denoising score matching. During training, the model learns to transform noisy samples back to clean samples. This is achieved by minimizing the discrepancy between the model's predicted score (gradient of the log-likelihood) and the true score of the data distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3.Forward and Backward Diffusion\n",
    "\n",
    "The diffusion process involves both a forward pass (generating samples) and a backward pass (learning the transformation parameters). In the forward pass, samples are generated by applying the learned transformation to noise. In the backward pass, the model is trained by optimizing its parameters to invert the diffusion process and map generated samples back to the original data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Common Models includes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise-Contrastive Priors (NCP):\n",
    "\n",
    "Key Concepts: Utilizes a noise-contrastive estimation framework for learning generative models. It introduces a diffusion process to model the data distribution.\n",
    "Reference: \"Noise-Contrastive Priors for Functional Uncertainty\" by Hanjun Dai, Ying Nian Wu, and Le Song."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denoising Score Matching (DSM):\n",
    "\n",
    "Key Concepts: Combines denoising autoencoders and score matching. It formulates generative modeling as the process of denoising samples through a diffusion process.\n",
    "Reference: \"Training Energy-Based Models via Denoising Score Matching\" by Yujia Li, Kevin Swersky, and Rich Zemel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffusion Models (DDPM):\n",
    "\n",
    "Key Concepts: Introduces a probabilistic diffusion process for generative modeling. The model is trained to denoise samples generated by the diffusion process.\n",
    "Reference: \"Improved Denoising Score Matching for Sampling from Intractable Distributions\" by Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel.\n",
    "\n",
    "\n",
    "Score-Based Generative Models:\n",
    "\n",
    "Key Concepts: Score-based generative models, including score matching and denoising score matching, can be considered diffusion models. They focus on learning a model that matches the score (gradient of the log-likelihood) of the true data distribution.\n",
    "Reference: \"Score-Based Generative Modeling through Stochastic Differential Equations\" by Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud.\n",
    "\n",
    "\n",
    "Stein Variational Gradient Descent (SVGD):\n",
    "\n",
    "Key Concepts: Utilizes Stein variational gradient descent to iteratively transport particles in the space, effectively performing a diffusion process.\n",
    "Reference: \"Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm\" by Qiang Liu and Dilin Wang.\n",
    "\n",
    "\n",
    "Variational Diffusion Autoencoder (VDAE):\n",
    "\n",
    "Key Concepts: Combines variational autoencoders (VAEs) with a diffusion process for generative modeling. It introduces a variational posterior to approximate the intractable posterior distribution.\n",
    "Reference: \"Variational Diffusion Autoencoder: Unsupervised Learning of Disentangled Features from Raw Data\" by Jan Hendrik Metzen, Mikołaj Bińkowski, Aaron van den Oord, and Roland Vollgraf."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
